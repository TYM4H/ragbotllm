import requests
import chromadb
from pypdf import PdfReader
import tempfile
import re

# --- Persistent –∫–ª–∏–µ–Ω—Ç Chroma ---
client = chromadb.PersistentClient(path="./chroma_db")


def get_user_collection(user_id):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–∏–ª–∏ —Å–æ–∑–¥–∞—ë—Ç) –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    return client.get_or_create_collection(f"user_{user_id}")


# --- –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ Ollama API ---
def ollama_embed(text: str):
    """–ü–æ–ª—É—á–∞–µ—Ç embedding –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ Ollama"""
    r = requests.post("http://localhost:11434/api/embeddings", json={
        "model": "nomic-embed-text",
        "prompt": text
    })
    if not r.ok:
        raise RuntimeError(f"Ollama embeddings error: {r.text}")
    return r.json()["embedding"]


# --- –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –º—É—Å–æ—Ä–∞ ---
def clean_text(t: str) -> str:
    t = re.sub(r'\s+', ' ', t)
    t = re.sub(r'[^\w\s.,;:!?()\-\[\]]', '', t)
    return t.strip()


# --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ PDF –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ ---
async def ingest_pdf(user_id, file):
    """–ß–∏—Ç–∞–µ—Ç PDF, –¥–µ–ª–∏—Ç –Ω–∞ —á–∞–Ω–∫–∏, –≤–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ Chroma"""
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(await file.read())
        tmp_path = tmp.name

    reader = PdfReader(tmp_path)
    text = "\n".join(page.extract_text() or "" for page in reader.pages)
    text = clean_text(text)

    # –¥–µ–ª–∏–º –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º, —á—Ç–æ–±—ã –Ω–µ —Ä–≤–∞—Ç—å —Å–º—ã—Å–ª
    sentences = re.split(r'(?<=[.!?])\s+', text)
    chunks = []
    chunk = ""
    for s in sentences:
        if len(chunk) + len(s) < 1000:
            chunk += " " + s
        else:
            chunks.append(chunk.strip())
            chunk = s
    if chunk:
        chunks.append(chunk.strip())

    collection = get_user_collection(user_id)
    print(f"üìÑ –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º {len(chunks)} —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")

    for i, chunk in enumerate(chunks):
        emb = ollama_embed(chunk)
        collection.add(
            ids=[f"{user_id}_{i}"],
            embeddings=[emb],
            documents=[chunk],
            metadatas=[{"source": "pdf"}]
        )

    print("‚úÖ –î–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω.")


# --- –û—Å–Ω–æ–≤–Ω–æ–π RAG-–∑–∞–ø—Ä–æ—Å ---
async def query_rag(user_id, question):
    """–ù–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏ –æ—Ç–≤–µ—á–∞–µ—Ç —á–µ—Ä–µ–∑ LLM"""
    collection = get_user_collection(user_id)
    q_emb = ollama_embed(question)

    results = collection.query(
        query_embeddings=[q_emb],
        n_results=10,
        include=["documents", "distances"]
    )

    pairs = list(zip(results["documents"][0], results["distances"][0]))
    pairs.sort(key=lambda x: x[1])  # —á–µ–º –º–µ–Ω—å—à–µ distance, —Ç–µ–º –±–ª–∏–∂–µ

    # –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø-4 –±–ª–∏–∂–∞–π—à–∏—Ö —á–∞–Ω–∫–∞
    top_docs = [d for d, _ in pairs[:4]]

    # —á–∏—Å—Ç–∏–º –∏ —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏
    seen = set()
    clean_docs = []
    for d in top_docs:
        d = d.replace('\n', ' ').replace('\uf02d', '-').strip()
        if d not in seen:
            seen.add(d)
            clean_docs.append(d)

    context = "\n\n".join(clean_docs)

    print("\nüîç –ù–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (distance ‚Üë):")
    for doc, dist in pairs[:6]:
        print(f"‚Ä¢ dist={dist:.4f} ‚Üí {doc[:120]}...")
    print("\n>>> CONTEXT SENT TO LLM:\n", context[:1000], "\n")

    prompt = f"""
–¢—ã ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ –ø–æ –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω–æ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É.
–ï—Å–ª–∏ –æ—Ç–≤–µ—Ç —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω, –ø–µ—Ä–µ—Å–∫–∞–∂–∏ –µ–≥–æ —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ –ø–æ-—Ä—É—Å—Å–∫–∏.
–ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç ‚Äî —Å–∫–∞–∂–∏: "–í –¥–æ–∫—É–º–µ–Ω—Ç–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± —ç—Ç–æ–º."

=== –ö–û–ù–¢–ï–ö–°–¢ ===
{context}

=== –í–û–ü–†–û–° ===
{question}
"""

    r = requests.post("http://localhost:11434/api/generate", json={
        "model": "llama3",
        "prompt": prompt,
        "stream": False
    })
    if not r.ok:
        raise RuntimeError(f"Ollama generation error: {r.text}")

    answer = r.json()["response"].strip()
    print("ü§ñ –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:", answer[:300])
    return answer
